@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@inproceedings{hu2016segmentation,
  title={Segmentation from natural language expressions},
  author={Hu, Ronghang and Rohrbach, Marcus and Darrell, Trevor},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{kazemzadeh2014referitgame,
  title={{ReferItGame}: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={EMNLP},
  year={2014}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L. and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C. and Berg, Tamara L.},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{liu2023gres,
  title={{GRES}: Generalized referring expression segmentation},
  author={Liu, Chang and Ding, Henghui and Jiang, Xudong},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{liu2017recurrent,
  title={Recurrent multimodal interaction for referring image segmentation},
  author={Liu, Chenxi and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Yuille, Alan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{li2018referring,
  title={Referring image segmentation via recurrent refinement networks},
  author={Li, Ruiyu and Li, Kaican and Kuo, Yi-Chun and Shu, Michelle and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{chen2019see,
  title={See-through-text grouping for referring image segmentation},
  author={Chen, Ding-Jie and Jia, Songhao and Lo, Yi-Chen and Chen, Hwann-Tzong and Liu, Tyng-Luh},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{ye2019cross,
  title={Cross-modal self-attention network for referring image segmentation},
  author={Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Wang, Yang},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{feng2021encoder,
  title={Encoder fusion network with co-attention embedding for referring image segmentation},
  author={Feng, Guang and Hu, Zhiwei and Zhang, Lihe and Lu, Huchuan},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{jing2021locate,
  title={Locate then segment: A strong pipeline for referring image segmentation},
  author={Jing, Ya and Kong, Tao and Wang, Wei and Wang, Liang and Li, Lei and Tan, Tieniu},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{wang2022cris,
  title={{CRIS}: {CLIP}-driven referring image segmentation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{kim2022restr,
  title={{ReSTR}: Convolution-free referring image segmentation using transformers},
  author={Kim, Namyup and Kim, Dongwon and Lan, Cuiling and Zeng, Wenjun and Kwak, Suha},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2022lavt,
  title={{LAVT}: Language-aware vision transformer for referring image segmentation},
  author={Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip H.S.},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yan2023universal,
  title={Universal instance perception as object discovery and retrieval},
  author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NACCL},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models}, 
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year={2023}
}

@inproceedings{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{zhu2024minigpt,
  title={{MiniGPT-4}: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{dai2023instructblip,
  title={Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle={ICML},
  year={2021}
}

@inproceedings{tong2024eyes,
  title={Eyes wide shut? {E}xploring the visual shortcomings of multimodal {LLMs}},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  booktitle={CVPR},
  year={2024}
}

@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal {LLMs}},
  author={Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Austin Wang and Rob Fergus and Yann LeCun and Saining Xie},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@inproceedings{li2023evaluating,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{sun2024aligning,
  title={Aligning large multimodal models with factually augmented {RLHF}},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and Keutzer, Kurt and Darrell, Trevor},
  booktitle={ACL Findings},
  year={2024}
}

@inproceedings{peng2024grounding,
  title={{Kosmos-2}: Grounding multimodal large language models to the world},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Ye, Qixiang and Wei, Furu},
  booktitle={ICLR},
  year={2024}
}

@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal {LLM}'s Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@inproceedings{wang2023visionllm,
  title={{VisionLLM}: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and Dai, Jifeng},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{pi2023detgpt,
  title={{DetGPT}: Detect what you need via reasoning},
  author={Pi, Renjie and Gao, Jiahui and Diao, Shizhe and Pan, Rui and Dong, Hanze and Zhang, Jipeng and Yao, Lewei and Han, Jianhua and Xu, Hang and Kong, Lingpeng and Zhang, Tong},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{you2024ferret,
  title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
  author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{li2024covlm,
  title={{CoVLM}: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding},
  author={Li, Junyan and Chen, Delin and Hong, Yining and Chen, Zhenfang and Chen, Peihao and Shen, Yikang and Gan, Chuang},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{lai2024lisa,
  title={{LISA}: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{rasheed2024glamm,
  title={{GLaMM}: Pixel grounding large multimodal model},
  author={Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M. and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S.},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhang2024groundhog,
  title={{Groundhog}: Grounding large language models to holistic segmentation},
  author={Zhang, Yichi and Ma, Ziqiao and Gao, Xiaofeng and Shakiah, Suhaila and Gao, Qiaozi and Chai, Joyce},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{ren2024pixellm,
  title={{PixelLM}: Pixel reasoning with large multimodal model},
  author={Ren, Zhongwei and Huang, Zhicheng and Wei, Yunchao and Zhao, Yao and Fu, Dongmei and Feng, Jiashi and Jin, Xiaojie},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chng2024mask,
  title={Mask grounding for referring image segmentation},
  author={Chng, Yong Xien and Zheng, Henry and Han, Yizeng and Qiu, Xuchong and Huang, Gao},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xia2024gsva,
  title={{GSVA}: Generalized segmentation via multimodal large language models},
  author={Xia, Zhuofan and Han, Dongchen and Han, Yizeng and Pan, Xuran and Song, Shiji and Huang, Gao},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{xu2024ullava,
  title={{u-LLaVA}: Unifying Multi-Modal Tasks via Large Language Model},
  author={Xu, Jinjin and Xu, Liwu and Yang, Yuzhe and Li, Xiang and Wang, Fanyi and Xie, Yanchun and Huang, Yi-Jie and Li, Yaqian},
  booktitle={ECAI},
  year={2024}
}

@inproceedings{chen2024sam4mllm,
  title={{SAM4MLLM}: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation},
  author={Chen, Yi-Chia and Li, Wei-Hua and Sun, Cheng and Wang, Yu-Chiang Frank and Chen, Chu-Song},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{zhang2024psalm,
  title={{PSALM}: Pixelwise segmentation with large multi-modal model},
  author={Zhang, Zheng and Ma, Yeyao and Zhang, Enming and Bai, Xiang},
  booktitle={ECCV},
  year={2024}
}

@article{luo2024hdc,
  title={{HDC}: Hierarchical Semantic Decoding with Counting Assistance for Generalized Referring Expression Segmentation},
  author={Luo, Zhuoyan and Wu, Yinghao and Liu, Yong and Xiao, Yicheng and Zhang, Xiao-Ping and Yang, Yujiu},
  journal={arXiv preprint arXiv:2405.15658},
  year={2024}
}


@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{cherti2023reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={CVPR},
  year={2023}
}

@article{oquab2023dinov2,
  title={{DINO}v2: Learning robust visual features without supervision},
  author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  journal={TMLR},
  year={2023}
}

@inproceedings{dosovitskiy2021image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{sun2021rethinking,
  title={Rethinking transformer-based set prediction for object detection},
  author={Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris M.},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
  booktitle={ICCV},
  year={2023}
}

@article{ravi2024sam,
  title={{SAM} 2: Segment anything in images and videos},
  author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman Rädle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Dollár and Christoph Feichtenhofer},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@inproceedings{zou2024segment,
  title={Segment everything everywhere all at once},
  author={Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{lin2014microsoft,
  title={Microsoft {COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{gupta2019lvis,
  title={{LVIS}: A dataset for large vocabulary instance segmentation},
  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
  journal={IJCV},
  volume={123},
  pages={32--73},
  year={2017}
}

@inproceedings{qi2023high,
  title={High Quality Entity Segmentation},
  author={Qi, Lu and Kuen, Jason and Shen, Tiancheng and Gu, Jiuxiang and Li, Wenbo and Guo, Weidong and Jia, Jiaya and Lin, Zhe and Yang, Ming-Hsuan},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zong2023detrs,
  title={{DETRs} with collaborative hybrid assignments training},
  author={Zong, Zhuofan and Song, Guanglu and Liu, Yu},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{rezatofighi2017deepsetnet,
  title={{DeepSetNet}: Predicting sets with deep neural networks},
  author={Rezatofighi, S. Hamid and B.G., Vijay Kumar and Milan, Anton and Abbasnejad, Ehsan and Dick, Anthony and Reid, Ian},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{vinyals2015pointer,
  title={Pointer networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle={NeurIPS},
  year={2015}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={CVPR},
  year={2024}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing {GPT-4} with 90\%* {ChatGPT} Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}

@inproceedings{liu2022convnet,
  title={A {ConvNet} for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{cui2024survey,
  title={A survey on multimodal large language models for autonomous driving},
  author={Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
  booktitle={WACV},
  year={2024}
}


@inproceedings{shao2024lmdrive,
  title={{LMDrive}: Closed-loop end-to-end driving with large language models},
  author={Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{liu2024vision,
  title={Vision-language model-driven scene understanding and robotic object manipulation},
  author={Liu, Sichao and Zhang, Jianjing and Gao, Robert X. and Wang, Xi Vincent and Wang, Lihui},
  booktitle={CASE},
  year={2024},
}

@inproceedings{gao2024physically,
  title={Physically grounded vision-language models for robotic manipulation},
  author={Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={ICRA},
  year={2024},
}

@article{konenkov2024vr,
  title={{VR-GPT}: Visual language model for intelligent virtual reality applications},
  author={Konenkov, Mikhail and Lykov, Artem and Trinitatova, Daria and Tsetserukou, Dzmitry},
  journal={arXiv preprint arXiv:2405.11537},
  year={2024}
}

@article{shen2024empowering,
  title={Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations},
  author={Shen, Tiancheng and Liew, Jun Hao and Mai, Long and Qi, Lu and Feng, Jiashi and Jia, Jiaya},
  journal={arXiv preprint arXiv:2406.00121},
  year={2024}
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  year={2019}
}

@article{wang2024segllm,
  title={{SegLLM}: Multi-round Reasoning Segmentation},
  author={Wang, XuDong and Zhang, Shaolun and Li, Shufan and Kallidromitis, Konstantinos and Li, Kehan and Kato, Yusuke and Kozuka, Kazuki and Darrell, Trevor},
  journal={arXiv preprint arXiv:2410.18923},
  year={2024}
}

@article{xu2024survey,
  title={A survey on robotics with foundation models: toward embodied {AI}},
  author={Xu, Zhiyuan and Wu, Kun and Wen, Junjie and Li, Jinming and Liu, Ning and Che, Zhengping and Tang, Jian},
  journal={arXiv preprint arXiv:2402.02385},
  year={2024}
}

@inproceedings{nguyen2023visual,
  title={Visual instruction inversion: Image editing via image prompting},
  author={Nguyen, Thao and Li, Yuheng and Ojha, Utkarsh and Lee, Yong Jae},
  booktitle={NeurIPS},
  year={2023}
}

@article{guo2024prompthis,
  title={{PrompTHis}: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation},
  author={Guo, Yuhan and Shao, Hanning and Liu, Can and Xu, Kai and Yuan, Xiaoru},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2024}
}

@software{adobephotoshop,
  author = {{Adobe Inc.}},
  title = {{Photoshop: Generative Fill}},
  url = {https://www.adobe.com/products/photoshop/generative-fill.html},
  year={2023},
  version = {CC 2023},
  date = {2023-03-06},
}

@inproceedings{kingma2015adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P. and Jimmy Ba},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{yu2023convolutions,
  title={Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional {CLIP}},
  author={Yu, Qihang and He, Ju and Deng, Xueqing and Shen, Xiaohui and Chen, Liang-Chieh},
  journal={NeurIPS},
  year={2023}
}

@article{qi2022open,
  title={Open world entity segmentation},
  author={Qi, Lu and Kuen, Jason and Wang, Yi and Gu, Jiuxiang and Zhao, Hengshuang and Torr, Philip and Lin, Zhe and Jia, Jiaya},
  journal={TPAMI},
  volume={45},
  number={7},
  pages={8743--8756},
  year={2022}
}

@inproceedings{kirillov2019panoptic,
  title={Panoptic segmentation},
  author={Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ding2022decoupling,
  title={Decoupling zero-shot semantic segmentation},
  author={Ding, Jian and Xue, Nan and Xia, Gui-Song and Dai, Dengxin},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{liang2023open,
  title={Open-vocabulary semantic segmentation with mask-adapted {CLIP}},
  author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{han2023open,
  title={Open-vocabulary semantic segmentation with decoupled one-pass network},
  author={Han, Cong and Zhong, Yujie and Li, Dengjie and Han, Kai and Ma, Lin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yu2024towards,
  title={Towards Open-Ended Visual Recognition with Large Language Models},
  author={Yu, Qihang and Shen, Xiaohui and Chen, Liang-Chieh},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{guo2024regiongpt,
  title={{RegionGPT}: Towards region understanding vision language model},
  author={Guo, Qiushan and De Mello, Shalini and Yin, Hongxu and Byeon, Wonmin and Cheung, Ka Chun and Yu, Yizhou and Luo, Ping and Liu, Sifei},
  booktitle={CVPR},
  year={2024}
}
