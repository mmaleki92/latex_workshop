\section{Conclusion}
\label{sec:con}
This work introduces a novel task, omnimodal referring expression segmentation (ORES), which extends RES with more sophisticated interactions through visual and textual prompts. We present a simple yet effective solution, \ourmodel, to achieve complex multimodal comprehension of segmentation masks. We demonstrate state-of-the-art performance compared to various baselines, not only in ORES but also in existing tasks (\ie, RES and GRES).

Future directions include leveraging the potentials of LMMs to enable capabilities such as generating textual justifications for predicted mask groups and supporting multi-round interactions. Furthermore, we plan to explore improving the synergy between the segmentation model and the LMM, along with developing compact models variants tailored specifically for ORES, based on computation-efficient LLMs.
