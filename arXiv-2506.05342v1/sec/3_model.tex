\section{Refer to Any Segmentation Mask Group}
\label{sec:model}

\begin{figure*}[ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{fig/model.pdf}
    \caption{\textbf{Overview of our Refer to Any Segmentation Mask Group (\ourmodel) framework.} We extend LLaVA-1.5~\cite{liu2024improved} with a segmentation model, a visual encoder ensemble, mask tokenization, and a binary selection classifier for mask grouping. The decoding procedure of the LLM is non-autoregressive~\cite{carion2020end}, as the input tokens are given as candidate mask tokens rather than predicted from previous tokens.}
    \label{fig:model}
    \vspace{-2mm}
\end{figure*}

Extending the decoupling strategy in open-vocabulary segmentation~\cite{liang2023open, han2023open, yu2024towards}, our approach leverages a segmentation model to propose candidate masks for a given image. As the segmentation model does not directly comprehend the complex vision-language prompt, we design a \emph{mask-centric} large multimodal model (LMM) to address the new challenge of understanding and grouping these masks. Our proposed framework, \textbf{Refer to Any Segmentation Mask Group} (\textbf{\ourmodel}, illustrated in Figure~\ref{fig:model}), includes several specialized components: a segmentation model that proposes candidates, a mask projector that encodes mask features, a binary selection classifier that determines which masks to include, and a non-autoregressive decoding procedure for more effective model optimization.
We introduce the model designs in Section~\ref{sec:model-arch} and training procedure in Section~\ref{sec:model-train}.

\subsection{Architecture Designs}
\label{sec:model-arch}

\noindent\textbf{LMM as meta-architecture.} The widely adopted LMM architecture~\cite{liu2023visual} uses a CLIP visual encoder~\cite{radford2021learning} to extract features from a given image, and then maps the visual features into the language feature space via a lightweight \emph{image projector}. The converted visual tokens are concatenated with text tokens to form a sequence and fed into a large language model (LLM) to generate output responses autoregressively~\cite{radford2018improving}. Although LMMs have acquired strong image-level vision-language capabilities, they are not originally designed for tasks focusing on understanding fine-grained visual entities. Therefore, to perform the mask grouping task, we enhance LLaVA-1.5~\cite{liu2024improved} (finetuned from Vicuna-13B~\cite{vicuna2023}) with the ability to encode mask representations and select masks according to input prompts.

\noindent\textbf{Mask tokenization.} After segmenting the image into candidate masks, we tokenize the masks into individual elements for the LLM to understand. Given a segmentation mask (either proposed by segmentation models or specified by users) plus feature maps extracted by visual encoders from the entire image, we perform \emph{mask pooling} to aggregate visual features within the mask. More specifically, the mask is downsampled to the same spatial size as each visual feature map, and visual features within the downsampled mask are averaged to produce the mask-level feature. Then, a lightweight \emph{mask projector} converts the concatenated mask-level features into the language feature space, and finally we consider these converted features as mask tokens. This procedure is depicted in the mask tokenization block in Figure~\ref{fig:model}. Furthermore, we prepend a learnable special token \texttt{\textless mask-pool-pre\textgreater} to each token that corresponds to a candidate mask. This special token indicates that the following token will be a mask token converted from a continuous embedding of a mask in the pool of candidates. These mask tokens are concatenated with the global visual tokens and text tokens as the LLM inputs.

\noindent\textbf{Reference mask representation.} In the input, reference masks are encoded similarly to the candidate masks. For instance, in the prompt ``Select all objects with the same color as \texttt{\textless mask-ref\textgreater},'' the special token \texttt{\textless mask-ref\textgreater} would be replaced by the actual token of the reference mask specified by the user. We can reuse the mask tokenization for encoding candidate mask tokens, but one minor difference is that we prepend different special tokens to candidate mask tokens and reference mask tokens, in order to distinguish their roles. For candidate masks, we prepend \texttt{\textless mask-pool-pre\textgreater} to indicate that the next token will be an embedding for a candidate mask, and the model should interpret it as one possible choice for the mask grouping task. For reference masks, we prepend \texttt{\textless mask-ref-pre\textgreater} to indicate that the next token will be a reference mask embedding, which allows the model to extract related information for the mask grouping prompt.

\noindent\textbf{Visual feature ensemble.} Recent studies~\cite{tong2024eyes,tong2024cambrian} suggest that the CLIP visual encoder~\cite{radford2021learning} has several inherent weaknesses such as unlocalized features. Therefore, CLIP alone is not ideal for our mask grouping task, as we need \emph{localized features} to represent and distinguish different masks. Following Cambrian-1~\cite{tong2024cambrian}, we employ an ensemble of four visual encoders: CLIP~\cite{radford2021learning}, SigLIP~\cite{zhai2023sigmoid}, ConvNeXt-based CLIP~\cite{liu2022convnet, cherti2023reproducible}, and DINOv2~\cite{oquab2023dinov2}. In addition, we create a position-aware feature map from 2D sinusoidal position embeddings~\cite{dosovitskiy2021image} to explicitly encode positions. To tokenize each mask, after computing mask-pooled features from all encoders, we concatenate them all to produce an aggregated mask feature, and then use the mask projector to map the feature into a language-aligned token. Since the mask pooling operation is performed per feature map, we allow different resolutions for each visual encoder, which better preserves the original capabilities of each encoder.

\noindent\textbf{Mask group decoding.} Given a set of candidate masks, our model needs to predict a subset (group) of masks according to the prompt. For mask group prediction, a straightforward solution may be directly predicting continuous mask embeddings one by one in an autoregressive manner~\cite{lai2024lisa, xia2024gsva, zhang2024groundhog}. However, this is challenging because a) LLMs are originally trained to model a distribution over discrete tokens, and b) learning to predict an unordered set is inherently hard due to the instable bipartite matching between predictions and the ground truth~\cite{sun2021rethinking}. For consistency with the discrete nature of LLMs and effective model optimization, we reformulate the mask group prediction problem as a \emph{per-mask binary classification} problem (Figure~\ref{fig:model}). More specifically, we learn to make a binary prediction for each candidate mask to indicate whether it should be included in the mask group based on the input prompt. We first provide all the tokens that encode the context, and then feed the candidate mask tokens again to the LLM to capture their output hidden states. Leveraging the strong semantic understanding and reasoning capabilities of the LLM, the output hidden state can indicate whether a candidate mask is positively related to the user prompt. Finally, a learnable \emph{binary selection classifier} is applied on top of the hidden states to produce the binary predictions.

Note that our LLM decoding does not follow the autoregressive paradigm---the inference-time input to the LLM is fixed as the candidate mask tokens, instead of using the previous output tokens. As we will show in Section~\ref{sec:expr-ablation}, this simple and direct decoding strategy greatly outperforms traditional autoregressive decoding. Meanwhile, we can perform binary classification on all candidate masks in one pass to boost inference efficiency, while autoregressive generation can output only one token at a time.

\subsection{Multi-stage Training}
\label{sec:model-train}

\noindent\textbf{Training stages.} To efficiently train the model containing both pretrained weights and randomly initialized weights, we divide the training into two stages following the practice of LLaVA~\cite{liu2023visual}. The first stage is mask projector pretraining, during which we freeze all modules except the mask projector, as it is a new module that cannot inherit from LLaVA model weights.
We design a pretext task where we provide \emph{only mask tokens and text prompts} (without global visual tokens) to the LLM and let it predict the image-level description, which is similar to the original LLaVA pretraining task but uses mask tokens instead.
This task aligns mask tokens with the LLM, so that they can jointly produce image descriptions. We find it viable to \emph{caption images using only mask tokens} and the loss converges to a level similar to LLaVA pretraining, because mask tokens can capture and describe the major objects, which is enough to caption object-centric images. For this stage, we reuse the pretraining data from LLaVA, a set of image-caption pairs, and add SAM-generated~\cite{kirillov2023segment} masks to each image.

In the second stage, visual instruction tuning, we finetune all modules except the visual encoders for the mask grouping task. Given an image and a set of candidate masks, the model learns to predict the correct subset of masks based on the input vision-language prompt. The model can be further finetuned on task-specific data to adapt the model for greater specialization in downstream tasks like RES.

\noindent\textbf{Training objectives.} In the pretraining stage, the expected model outputs are text-only, so we can train the model using cross-entropy loss~\cite{liu2023visual, radford2018improving}. During the visual instruction tuning stage, the learning task transitions to mask grouping. Therefore, we change the training objective from maximizing the likelihood of the caption to optimizing for per-mask binary classification.
More specifically, we compute the mean binary cross-entropy loss, averaged over all candidate masks. Among numerous mask candidates, usually only a few should be selected in the group. Due to this imbalanced distribution of positive and negative samples, we assign a larger loss weight to positive candidates.
