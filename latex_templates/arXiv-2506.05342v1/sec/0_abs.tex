\begin{strip}
    \centering
    \vspace*{-10mm}
    \includegraphics[width=\linewidth]{fig/teaser.pdf}
    \captionof{figure}{\textbf{Omnimodal referring expression segmentation (ORES) according to arbitrary vision-language prompts.} \textit{(Left)} Our approach, Refer to Any Segmentation Mask Group (\ourmodel), can understand a complex text prompt involving multiple conditions. \textit{(Middle)} Reference visual entities can be included as visual prompts to enhance expressivity, addressing the challenge of describing the same details using language alone. \textit{(Right)} The grouped segmentation masks conveniently enable various fine-grained downstream applications, such as object removal and editing. In each pair of images, the left one is the input and the right one is the output. Best viewed on an electronic device with zoom-in functionality.}
    \label{fig:teaser}
\end{strip}

\begin{abstract}
Recent image segmentation models have advanced to segment images into high-quality masks for visual entities\footnote{Countable objects and amorphous stuff regions~\cite{kirillov2019panoptic, qi2022open}.}, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts.
To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to ``Refer to Any Segmentation Mask Group'' (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model.
For training and benchmarking ORES models, we create datasets \ourlargedata and \ourgooddata to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks.
Project page: \url{https://Ref2Any.github.io}.
\end{abstract}
