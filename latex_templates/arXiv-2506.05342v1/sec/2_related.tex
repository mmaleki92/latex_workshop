\section{Related Work}
\label{sec:related}

\noindent\textbf{Referring expression segmentation (RES)} aims to segment one object at a time based on descriptions in natural language~\cite{hu2016segmentation, kazemzadeh2014referitgame, mao2016generation, yu2016modeling}. Earlier approaches focused on combining visual and language features~\cite{liu2017recurrent, li2018referring, chen2019see, ye2019cross, feng2021encoder, jing2021locate} and incorporating transformer models~\cite{wang2022cris, kim2022restr, yang2022lavt, yan2023universal}. Recent advancements~\cite{liu2023gres} have expanded the classic RES task to include multi-target and no-target queries, referred to as generalized RES (GRES). Building on this progress, our work further enhances GRES by enabling more effective and user-friendly interactions through the flexible integration of visual and textual inputs. 
 
\noindent\textbf{Large multimodal models (LMMs)} extend large language models (LLMs)~\cite{devlin2019bert, radford2018improving, touvron2023llama} with vision-language capabilities via visual instruction tuning~\cite{liu2023visual, zhu2024minigpt, dai2023instructblip}. Early LMMs are mainly based on CLIP~\cite{radford2021learning} patch-level visual features and show weaknesses in object-level comprehension and reasoning~\cite{tong2024eyes, tong2024cambrian, li2023evaluating, sun2024aligning}. LMMs can be equipped with grounding capabilities for generating bounding boxes~\cite{peng2024grounding, chen2023shikra, wang2023visionllm, pi2023detgpt, you2024ferret, li2024covlm} or segmentation masks~\cite{lai2024lisa, rasheed2024glamm, zhang2024groundhog, ren2024pixellm} via training on converted datasets with semantic-pixel alignment. Unlike prior LMMs, our \ourmodel is not trained for text generation, because text responses are unnecessary in the task and applications we consider (Figure~\ref{fig:teaser}), and high-quality mask groups are prioritized.

\noindent\textbf{Grounding LMMs} provide grounded vision-language understanding, and achieve state-of-the-art performance in RES and GRES~\cite{lai2024lisa, chng2024mask, zhang2024groundhog, rasheed2024glamm, xu2024ullava, chen2024sam4mllm, zhang2024psalm}. Among them, Groundhog~\cite{zhang2024groundhog} is most related to our work, which also performs RES by selecting from mask proposals. Our \ourmodel differs from Groundhog in these critical aspects: a) We adopt a non-autoregressive decoding procedure, outperforming the traditional autoregressive decoding used by Groundhog (Section~\ref{sec:expr-ablation}). b) We accept mask-based visual prompts for the complex ORES task, while for Groundhog, mask prompts are only effective in region description tasks.
